{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appartment Rent Ad Creation\n",
    "\n",
    "I have an appartment that I must rent. Let's see if TinyTroupe can help me advertise to the right people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for default config on: /workspaces/TinyTroupeLiteLLM/examples/../tinytroupe/utils/../config.ini\n",
      "Found custom config on: /workspaces/TinyTroupeLiteLLM/examples/config.ini\n",
      "\n",
      "!!!!\n",
      "DISCLAIMER: TinyTroupe relies on Artificial Intelligence (AI) models to generate content. \n",
      "The AI models are not perfect and may produce inappropriate or inacurate results. \n",
      "For any serious or consequential use, please review the generated content before using it.\n",
      "!!!!\n",
      "\n",
      "\n",
      "=================================\n",
      "Current TinyTroupe configuration \n",
      "=================================\n",
      "[LLM]\n",
      "provider = vertex_ai\n",
      "model = vertex_ai/gemini-2.0-flash\n",
      "max_tokens = 4000\n",
      "temperature = 1.2\n",
      "top_p = 1.0\n",
      "frequency_penalty = 0.0\n",
      "presence_penalty = 0.0\n",
      "timeout = 60\n",
      "max_attempts = 5\n",
      "waiting_time = 1\n",
      "exponential_backoff_factor = 5\n",
      "embedding_model = text-embedding-3-small\n",
      "cache_api_calls = False\n",
      "cache_file_name = litellm_api_cache.pickle\n",
      "enable_fallbacks = False\n",
      "fallback_models = gpt-3.5-turbo,claude-3-haiku-20240307\n",
      "\n",
      "[OpenAI]\n",
      "api_type = openai\n",
      "azure_api_version = 2024-08-01-preview\n",
      "model = gpt-4o-mini\n",
      "max_tokens = 4000\n",
      "temperature = 1.2\n",
      "freq_penalty = 0.0\n",
      "presence_penalty = 0.0\n",
      "timeout = 60\n",
      "max_attempts = 5\n",
      "waiting_time = 2\n",
      "exponential_backoff_factor = 5\n",
      "embedding_model = text-embedding-3-small\n",
      "cache_api_calls = False\n",
      "cache_file_name = openai_api_cache.pickle\n",
      "max_content_display_length = 1024\n",
      "litellm_model = \n",
      "azure_embedding_model_api_version = 2023-05-15\n",
      "\n",
      "[Simulation]\n",
      "rai_harmful_content_prevention = True\n",
      "rai_copyright_infringement_prevention = True\n",
      "\n",
      "[Logging]\n",
      "loglevel = ERROR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import tinytroupe\n",
    "from tinytroupe.agent import TinyPerson\n",
    "from tinytroupe.environment import TinyWorld, TinySocialNetwork\n",
    "from tinytroupe.examples import *\n",
    "from tinytroupe.extraction import ResultsExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_group = TinyWorld(\"Focus group\", [create_lisa_the_data_scientist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation = \\\n",
    "\"\"\" \n",
    "This is a focus group dedicated to finding the best way to advertise an appartment for rent.\n",
    "Everyone in the group is a friend to the person who is renting the appartment, called Paulo.\n",
    "The objective is to find the best way to advertise the appartment, so that Paulo can find a good tenant.\n",
    "\"\"\"\n",
    "\n",
    "apartment_description = \\\n",
    "\"\"\"\t\n",
    "The appartment has the following characteristics:\n",
    "  - It is in an old building, but was completely renovated and remodeled by an excellent architect. \n",
    "    There are almost no walls, so it is very spacious, mostly composed of integrated spaces. \n",
    "  - It was also recently repainted, so it looks brand new.\n",
    "  - 1 bedroom. Originally, it had two, but one was converted into a home office.\n",
    "  - 1 integrated kitchen and living room. The kitchen is very elegant, with a central eating wood table,\n",
    "    with 60s-style chairs. The appliances are in gray and steel, and the cabinets are in white, the wood\n",
    "    is light colored.\n",
    "  - Has wood-like floors in all rooms, except the kitchen and bathroom, which are tiled.  \n",
    "  - 2 bathrooms. Both with good taste porcelain and other decorative elements.\n",
    "  - 1 laundry room. The washing machine is new and also doubles as a dryer.\n",
    "  - Is already furnished with a bed, a sofa, a table, a desk, a chair, a washing machine, a refrigerator, \n",
    "    a stove, and a microwave.\n",
    "  - It has a spacious shelf for books and other objects.\n",
    "  - It is close to: a very convenient supermarket, a bakery, a gym, a bus stop, and a subway station. \n",
    "    It is also close to a great argentinian restaurant, and a pizzeria.\n",
    "  - It is located at a main avenue, but the appartment is in the back of the building, so it is very quiet.\n",
    "  - It is near of the best Medicine School in the country, so it is a good place for a medical student.  \n",
    "\"\"\"\n",
    "\n",
    "task = \\\n",
    "\"\"\"\n",
    "Discuss the best way to advertise the appartment, so that Paulo can find a good tenant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inform the focus groups about their situation and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"margin:0px;;white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic; text-decoration: underline\">USER</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\"> --&gt; </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic; text-decoration: underline\">Lisa Carter</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">: [CONVERSATION] </span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt;   This is a focus group dedicated to finding the best way to advertise an appartment for</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; rent. Everyone in the group is a friend to the person who is renting the appartment,</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; called Paulo. The objective is to find the best way to advertise the appartment, so that</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; Paulo can find a good tenant.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3;4;38;5;51mUSER\u001b[0m\u001b[1;3;38;5;51m --> \u001b[0m\u001b[1;3;4;38;5;51mLisa Carter\u001b[0m\u001b[1;3;38;5;51m: \u001b[0m\u001b[1;3;38;5;51m[\u001b[0m\u001b[1;3;38;5;51mCONVERSATION\u001b[0m\u001b[1;3;38;5;51m]\u001b[0m\u001b[1;3;38;5;51m \u001b[0m\n",
       "\u001b[1;3;38;5;51m          >   This is a focus group dedicated to finding the best way to advertise an appartment for\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > rent. Everyone in the group is a friend to the person who is renting the appartment,\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > called Paulo. The objective is to find the best way to advertise the appartment, so that\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > Paulo can find a good tenant.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"margin:0px;;white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic; text-decoration: underline\">USER</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\"> --&gt; </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic; text-decoration: underline\">Lisa Carter</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">: [CONVERSATION] </span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt;          The appartment has the following characteristics:   - It is in an old building,</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; but was completely renovated and remodeled by an excellent architect.      There are</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; almost no walls, so it is very spacious, mostly composed of integrated spaces.    - It</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; was also recently repainted, so it looks brand new.   - 1 bedroom. Originally, it had</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; two, but one was converted into a home office.   - 1 integrated kitchen and living room.</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; The kitchen is very elegant, with a central eating wood table,     with 60s-style</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; chairs. The appliances are in gray and steel, and the cabinets are in white, the wood</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; is light colored.   - Has wood-like floors in all rooms, except the kitchen and</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; bathroom, which are tiled.     - 2 bathrooms. Both with good taste porcelain and other</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; decorative elements.   - 1 laundry room. The washing machine is new and also doubles as</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; a dryer.   - Is already furnished with a bed, a sofa, a table, a desk, a chair, a</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; washing machine, a refrigerator,      a stove, and a microwave.   - It has a spacious</span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt; (...)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3;4;38;5;51mUSER\u001b[0m\u001b[1;3;38;5;51m --> \u001b[0m\u001b[1;3;4;38;5;51mLisa Carter\u001b[0m\u001b[1;3;38;5;51m: \u001b[0m\u001b[1;3;38;5;51m[\u001b[0m\u001b[1;3;38;5;51mCONVERSATION\u001b[0m\u001b[1;3;38;5;51m]\u001b[0m\u001b[1;3;38;5;51m \u001b[0m\n",
       "\u001b[1;3;38;5;51m          >          The appartment has the following characteristics:   - It is in an old building,\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > but was completely renovated and remodeled by an excellent architect.      There are\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > almost no walls, so it is very spacious, mostly composed of integrated spaces.    - It\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > was also recently repainted, so it looks brand new.   - \u001b[0m\u001b[1;3;38;5;51m1\u001b[0m\u001b[1;3;38;5;51m bedroom. Originally, it had\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > two, but one was converted into a home office.   - \u001b[0m\u001b[1;3;38;5;51m1\u001b[0m\u001b[1;3;38;5;51m integrated kitchen and living room.\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > The kitchen is very elegant, with a central eating wood table,     with 60s-style\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > chairs. The appliances are in gray and steel, and the cabinets are in white, the wood\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > is light colored.   - Has wood-like floors in all rooms, except the kitchen and\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > bathroom, which are tiled.     - \u001b[0m\u001b[1;3;38;5;51m2\u001b[0m\u001b[1;3;38;5;51m bathrooms. Both with good taste porcelain and other\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > decorative elements.   - \u001b[0m\u001b[1;3;38;5;51m1\u001b[0m\u001b[1;3;38;5;51m laundry room. The washing machine is new and also doubles as\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > a dryer.   - Is already furnished with a bed, a sofa, a table, a desk, a chair, a\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > washing machine, a refrigerator,      a stove, and a microwave.   - It has a spacious\u001b[0m\n",
       "\u001b[1;3;38;5;51m          > \u001b[0m\u001b[1;3;38;5;51m(\u001b[0m\u001b[1;3;38;5;51m...\u001b[0m\u001b[1;3;38;5;51m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"margin:0px;;white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic; text-decoration: underline\">USER</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\"> --&gt; </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic; text-decoration: underline\">Lisa Carter</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">: [CONVERSATION] </span>\n",
       "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; font-style: italic\">          &gt;  Discuss the best way to advertise the appartment, so that Paulo can find a good tenant.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3;4;38;5;51mUSER\u001b[0m\u001b[1;3;38;5;51m --> \u001b[0m\u001b[1;3;4;38;5;51mLisa Carter\u001b[0m\u001b[1;3;38;5;51m: \u001b[0m\u001b[1;3;38;5;51m[\u001b[0m\u001b[1;3;38;5;51mCONVERSATION\u001b[0m\u001b[1;3;38;5;51m]\u001b[0m\u001b[1;3;38;5;51m \u001b[0m\n",
       "\u001b[1;3;38;5;51m          >  Discuss the best way to advertise the appartment, so that Paulo can find a good tenant.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "focus_group.broadcast(situation)\n",
    "focus_group.broadcast(apartment_description)\n",
    "focus_group.broadcast(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the focus group discussion\n",
    "\n",
    "In this example, we'll use real LLM API calls with Google's Vertex AI (Gemini) model. This approach provides:\n",
    "\n",
    "1. **Dynamic responses** - Unique, real-time generated content based on your specific inputs\n",
    "2. **Full model capabilities** - Access to the model's complete reasoning and generation abilities\n",
    "3. **Realistic agent behavior** - Authentic TinyTroupe agent interactions as designed\n",
    "\n",
    "> **Note:** This notebook uses Google Cloud credentials to access Vertex AI. The API calls will incur charges based on your Google Cloud account's usage pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:19:52 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-1.5-pro; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model configuration: vertex_ai/gemini-2.0-flash with provider: vertex_ai\n",
      "Switching to vertex_ai/gemini-1.5-pro for this example\n",
      "Found credentials file at /workspaces/TinyTroupeLiteLLM/leaftix-c8add1acbffc.json\n",
      "Credentials loaded successfully for project: leaftix\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "Error connecting to Vertex AI: litellm.BadRequestError: VertexAIException BadRequestError - {\n",
      "  \"error\": {\n",
      "    \"code\": 403,\n",
      "    \"message\": \"Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/leaftix/locations/us-central1/publishers/google/models/gemini-1.5-pro' (or it may not exist).\",\n",
      "    \"status\": \"PERMISSION_DENIED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n",
      "        \"reason\": \"IAM_PERMISSION_DENIED\",\n",
      "        \"domain\": \"aiplatform.googleapis.com\",\n",
      "        \"metadata\": {\n",
      "          \"resource\": \"projects/leaftix/locations/us-central1/publishers/google/models/gemini-1.5-pro\",\n",
      "          \"permission\": \"aiplatform.endpoints.predict\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "Please check your credentials and internet connection.\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "Error connecting to Vertex AI: litellm.BadRequestError: VertexAIException BadRequestError - {\n",
      "  \"error\": {\n",
      "    \"code\": 403,\n",
      "    \"message\": \"Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/leaftix/locations/us-central1/publishers/google/models/gemini-1.5-pro' (or it may not exist).\",\n",
      "    \"status\": \"PERMISSION_DENIED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n",
      "        \"reason\": \"IAM_PERMISSION_DENIED\",\n",
      "        \"domain\": \"aiplatform.googleapis.com\",\n",
      "        \"metadata\": {\n",
      "          \"resource\": \"projects/leaftix/locations/us-central1/publishers/google/models/gemini-1.5-pro\",\n",
      "          \"permission\": \"aiplatform.endpoints.predict\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "Please check your credentials and internet connection.\n"
     ]
    }
   ],
   "source": [
    "# Configure TinyTroupe to use Google Vertex AI (Gemini) as the LLM provider\n",
    "import os\n",
    "import sys\n",
    "import tinytroupe.litellm_utils as litellm_utils\n",
    "from tinytroupe import utils\n",
    "import litellm\n",
    "import json\n",
    "from google.auth import credentials\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Show current model configuration\n",
    "config = utils.read_config_file()\n",
    "print(f\"Current model configuration: {config['LLM']['MODEL']} with provider: {config['LLM']['PROVIDER']}\")\n",
    "\n",
    "# Set up Google Vertex AI provider and model\n",
    "provider = \"vertex_ai\"\n",
    "model = \"gemini-1.5-pro\"  # Using Gemini 1.5 Pro for advanced capabilities\n",
    "print(f\"Switching to {provider}/{model} for this example\")\n",
    "\n",
    "# Path to the credentials file\n",
    "credentials_path = \"/workspaces/TinyTroupeLiteLLM/leaftix-c8add1acbffc.json\"\n",
    "\n",
    "# Configure Google Cloud credentials\n",
    "if os.path.exists(credentials_path):\n",
    "    print(f\"Found credentials file at {credentials_path}\")\n",
    "    # Set the environment variable for Google Cloud authentication\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "    \n",
    "    # Load the credentials to verify they're valid\n",
    "    try:\n",
    "        creds = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "        print(f\"Credentials loaded successfully for project: {creds.project_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading credentials: {e}\")\n",
    "else:\n",
    "    print(f\"Credentials file not found at {credentials_path}\")\n",
    "    raise FileNotFoundError(f\"Could not find credentials file at {credentials_path}\")\n",
    "\n",
    "# Force the model values directly in the litellm_utils module to ensure consistency\n",
    "litellm_utils.default[\"provider\"] = provider\n",
    "litellm_utils.default[\"model\"] = model\n",
    "litellm_utils.default[\"max_tokens\"] = int(config['LLM']['MAX_TOKENS'])\n",
    "\n",
    "# Verify the API connection\n",
    "try:\n",
    "    # Test with a simple completion\n",
    "    test_response = litellm.completion(\n",
    "        model=\"vertex_ai/gemini-1.5-pro\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello, are you working?\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    print(\"API connection successful! Response:\", test_response.choices[0].message.content)\n",
    "    \n",
    "    # Now run the focus group discussion with real LLM responses\n",
    "    focus_group.run(3)\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Vertex AI: {e}\")\n",
    "    print(\"Please check your credentials and internet connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:19:52 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:53,070 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:53,070 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:53,242 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:53,242 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:53,410 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:53,603 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:53,779 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:53,779 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:53,980 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:53 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:54,166 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:54 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:54,166 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:54 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:54,358 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:54 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:54 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:54,569 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:54 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:54,569 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:54 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:54,790 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:54 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:54 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:55,019 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:55 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:55 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:55,254 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:55 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:55 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:55,497 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:55 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:55 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:55,741 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:55 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:55 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:55,990 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:55 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:55 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:56,244 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:56 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:56 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:56,506 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:56 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:56 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:56,915 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:56 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:56 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:57,212 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:57 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:57,212 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:57 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:57,515 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:57 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:57 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:57,769 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:57 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:57 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:58,038 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:58 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:58 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:58,315 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:58 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:58 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:58,778 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:58 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:58 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:59,090 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:59 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:19:59,090 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:59 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:59,397 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:59 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:59 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:19:59,794 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:19:59 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:19:59 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:00,195 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:00 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:00 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:00,634 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:00 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:00 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:01,229 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:01 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:01 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:20:01,571 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:01 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "2025-07-12 19:20:01,571 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:01 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:01,926 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:01 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:01 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:02,296 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:02 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:02 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:02,763 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:02 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:02 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:03,179 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:03 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:03 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:03,714 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:03 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:03 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:04,340 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:04 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:04 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:04,829 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:04 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:04 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:05,477 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:05 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:05 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:05,951 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:05 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:05 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:06,589 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:06 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:06 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:07,239 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:07 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:07 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:07,865 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:07 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:07 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:08,294 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:08 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:08 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:09,081 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:09 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:09 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:09,855 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:09 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:09 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:10,532 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:10 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:10 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:11,319 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:11 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:11 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:12,235 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:12 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:12 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:12,801 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:12 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:12 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:20:13,229 - tinytroupe - ERROR - Not Found error with model gemini-1.5-pro: litellm.NotFoundError: VertexAIException - {\"detail\":\"Not Found\"}\n",
      "\u001b[92m19:20:13 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n",
      "\u001b[92m19:20:13 - LiteLLM:INFO\u001b[0m: utils.py:3225 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = vertex_ai\n"
     ]
    }
   ],
   "source": [
    "extractor = ResultsExtractor()\n",
    "\n",
    "extractor.extract_results_from_world(focus_group,\n",
    "                                     extraction_objective=\"Compose an advertisement copy based on the ideas given.\",\n",
    "                                     fields=[\"ad_copy\"],\n",
    "                                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.save_as_json(\"../data/extractions/appartment_rent_ad.extraction.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternative: Using OpenAI API\n",
    "\n",
    "If you prefer to use OpenAI's GPT models instead of Google Vertex AI, you can use the following configuration approach:\n",
    "\n",
    "```python\n",
    "# For OpenAI setup\n",
    "import os\n",
    "\n",
    "# 1. Set your OpenAI API key\n",
    "openai_api_key = \"your-openai-api-key\"  # Replace with your actual API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# 2. Set the provider and model\n",
    "provider = \"openai\"\n",
    "model = \"gpt-4\" # or \"gpt-3.5-turbo\" for a less expensive option\n",
    "\n",
    "# 3. Update litellm configuration\n",
    "litellm_utils.default[\"provider\"] = provider\n",
    "litellm_utils.default[\"model\"] = model\n",
    "\n",
    "# 4. Test the connection\n",
    "test_response = litellm.completion(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, are you working?\"}],\n",
    "    max_tokens=10\n",
    ")\n",
    "print(\"API connection successful! Response:\", test_response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "This configuration requires an OpenAI API key with appropriate permissions and quota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Google Vertex AI\n",
    "\n",
    "If you encounter issues with the Google Vertex AI API calls, consider the following:\n",
    "\n",
    "1. **Credentials Issues**: \n",
    "   - Ensure your service account JSON file is valid and has not been corrupted\n",
    "   - Verify the service account has the necessary permissions (Vertex AI User role)\n",
    "   - Check that the project has the Vertex AI API enabled\n",
    "\n",
    "2. **Common Vertex AI Errors**:\n",
    "   - `Permission denied`: The service account lacks required permissions\n",
    "   - `API not enabled`: Vertex AI API is not enabled for your project\n",
    "   - `Quota exceeded`: You've reached your API quota limits\n",
    "   - `Region not available`: The model may not be available in all regions\n",
    "\n",
    "3. **Response Format**: \n",
    "   - If TinyTroupe reports parsing errors, the LLM might not be returning the expected format\n",
    "   - Try adjusting model parameters (temperature, top_p) for more consistent outputs\n",
    "   - Gemini models generally need more explicit instructions for structured outputs\n",
    "\n",
    "4. **Environment Variables**:\n",
    "   - Verify `GOOGLE_APPLICATION_CREDENTIALS` is correctly set to your JSON file path\n",
    "   - If using a custom endpoint, ensure the correct region is specified\n",
    "\n",
    "5. **Network and Firewall Issues**:\n",
    "   - Check if outbound connections to Google APIs are allowed in your environment\n",
    "   - Verify your network allows communication with Google Cloud endpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TinyTroupeLiteLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
