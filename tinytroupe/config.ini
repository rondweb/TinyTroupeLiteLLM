[LLM]
#
# LiteLLM Configuration - Universal LLM Interface
#

# Default provider: openai, anthropic, cohere, replicate, etc.
PROVIDER=vertex_ai

# Default model (will be prefixed with provider if needed)
MODEL=vertex_ai/gemini-2.0-flash

# Universal parameters (LiteLLM handles provider translation)
MAX_TOKENS=4000
TEMPERATURE=1.2
TOP_P=1.0
FREQUENCY_PENALTY=0.0
PRESENCE_PENALTY=0.0
TIMEOUT=60
MAX_ATTEMPTS=5
WAITING_TIME=1
EXPONENTIAL_BACKOFF_FACTOR=5

# Embedding model
EMBEDDING_MODEL=text-embedding-3-small

# Caching settings
CACHE_API_CALLS=False
CACHE_FILE_NAME=litellm_api_cache.pickle

# Advanced LiteLLM features
ENABLE_FALLBACKS=False
FALLBACK_MODELS=gpt-3.5-turbo,claude-3-haiku-20240307

# Provider-specific API keys (set these in environment variables for security)
# OPENAI_API_KEY=your_key_here
# ANTHROPIC_API_KEY=your_key_here
# COHERE_API_KEY=your_key_here
# REPLICATE_API_KEY=your_key_here

# Legacy OpenAI section (for backward compatibility)
[OpenAI]
#
# OpenAI or Azure OpenAI Service (Legacy - use LLM section above)
#

# Default options: openai, azure
API_TYPE=openai

# Check Azure's documentation for updates here:
# https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line&pivots=programming-language-python
AZURE_API_VERSION=2023-05-15

#
# Model parameters
#

MODEL=gpt-4o-mini
MAX_TOKENS=4000
TEMPERATURE=1.2
FREQ_PENALTY=0.0
PRESENCE_PENALTY=0.0
TIMEOUT=60
MAX_ATTEMPTS=5
WAITING_TIME=1
EXPONENTIAL_BACKOFF_FACTOR=5

EMBEDDING_MODEL=text-embedding-3-small 

CACHE_API_CALLS=False
CACHE_FILE_NAME=openai_api_cache.pickle

MAX_CONTENT_DISPLAY_LENGTH=1024

[Simulation]
RAI_HARMFUL_CONTENT_PREVENTION=True
RAI_COPYRIGHT_INFRINGEMENT_PREVENTION=True

[Logging]
LOGLEVEL=ERROR
# ERROR
# WARNING
# INFO
# DEBUG